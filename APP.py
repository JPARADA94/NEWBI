import streamlit as st
import pandas as pd
from io import BytesIO
import unicodedata

# â€”â€”â€”â€”â€”â€” ConfiguraciÃ³n general â€”â€”â€”â€”â€”â€”
st.set_page_config(page_title="ValidaciÃ³n Global â€“ Excel Consolidado v4.2", layout="wide")
st.title("ðŸ“Š ValidaciÃ³n Global de Encabezados â€“ Excel Consolidado â€“ Mobil v4.2")
st.markdown("**Responsables:** Grupo de Soporte en Campo â€“ Mobil")

st.markdown(
    """
### ðŸ§¾ Instrucciones de uso:
1. Sube **uno o varios archivos Excel (.xlsx)**.
2. El sistema unirÃ¡ todos los archivos en un solo conjunto.
3. ValidarÃ¡ los encabezados sobre el conjunto completo.
4. GenerarÃ¡ dos reportes:
   - **Tabla de desalineaciones**: posiciÃ³n esperada vs. posiciÃ³n encontrada o ausencia.
   - **Tabla de columnas con datos no mapeadas** (se agregarÃ¡n al final).
5. Genera **un Ãºnico archivo Excel consolidado** y los reportes descargables.
"""
)

# â€”â€”â€”â€”â€”â€” Utilitarios â€”â€”â€”â€”â€”â€”
def col_index_to_letter(idx: int) -> str:
    """Convierte Ã­ndice base 0 a letra(s) de columna de Excel (A, Z, AA...)."""
    letter = ""
    while idx >= 0:
        letter = chr(idx % 26 + ord('A')) + letter
        idx = idx // 26 - 1
    return letter

def normalize_header(s: str) -> str:
    """Normaliza encabezados para coincidencias tolerantes."""
    if s is None:
        return ""
    s = s.strip()
    s = s.replace("â‰¥", ">=").replace("Îœ", "Âµ").replace("\u00A0", " ")  # NBSP â†’ espacio
    s = s.replace("**", "")
    s = unicodedata.normalize('NFKD', s)
    s = "".join(ch for ch in s if not unicodedata.combining(ch))
    return s.lower()

def df_to_xlsx_bytes(df: pd.DataFrame, sheet: str = "Hoja") -> BytesIO:
    """Convierte un DataFrame a bytes XLSX usando openpyxl (sin XlsxWriter)."""
    buf = BytesIO()
    with pd.ExcelWriter(buf, engine="openpyxl") as w:
        df.to_excel(w, index=False, sheet_name=sheet)
    buf.seek(0)
    return buf

def make_downloads(df: pd.DataFrame, base_name: str, sheet: str):
    """Botones de descarga CSV/XLSX para un DataFrame."""
    csv_bytes = df.to_csv(index=False).encode("utf-8-sig")
    xlsx_bytes = df_to_xlsx_bytes(df, sheet=sheet)
    c1, c2 = st.columns(2)
    c1.download_button(
        f"ðŸ“¥ {base_name} (CSV)",
        data=csv_bytes,
        file_name=f"{base_name}.csv",
        mime="text/csv",
    )
    c2.download_button(
        f"ðŸ“¥ {base_name} (XLSX)",
        data=xlsx_bytes,
        file_name=f"{base_name}.xlsx",
        mime="application/vnd.openxmlformats-officedocument.spreadsheetml.sheet",
    )

# â€”â€”â€” Orden base EXACTO solicitado (sin id_muestra) â€”â€”â€”
expected_names = [
    "NOMBRE_CLIENTE",
    "NOMBRE_OPERACION",
    "N_MUESTRA",
    "CORRELATIVO",
    "FECHA_MUESTREO",
    "FECHA_INGRESO",
    "FECHA_RECEPCION",
    "FECHA_INFORME",
    "EDAD_COMPONENTE",
    "UNIDAD_EDAD_COMPONENTE",
    "EDAD_PRODUCTO",
    "UNIDAD_EDAD_PRODUCTO",
    "CANTIDAD_ADICIONADA",
    "UNIDAD_CANTIDAD_ADICIONADA",
    "PRODUCTO",
    "TIPO_PRODUCTO",
    "EQUIPO",
    "TIPO_EQUIPO",
    "MARCA_EQUIPO",
    "MODELO_EQUIPO",          # â† posiciÃ³n 19
    "COMPONENTE",
    "MARCA_COMPONENTE",
    "MODELO_COMPONENTE",
    "DESCRIPTOR_COMPONENTE",
    "ESTADO",
    "NIVEL_DE_SERVICIO",
    "ÃNDICE PQ (PQI) - 3",
    "PLATA (AG) - 19",
    "ALUMINIO (AL) - 20",
    "CROMO (CR) - 24",
    "COBRE (CU) - 25",
    "HIERRO (FE) - 26",
    "TITANIO (TI) - 38",
    "PLOMO (PB) - 35",
    "NÃQUEL (NI) - 32",
    "MOLIBDENO (MO) - 30",
    "SILICIO (SI) - 36",
    "SODIO (NA) - 31",
    "POTASIO (K) - 27",
    "VANADIO (V) - 39",
    "BORO (B) - 18",
    "BARIO (BA) - 21",
    "CALCIO (CA) - 22",
    "CADMIO (CD) - 23",
    "MAGNESIO (MG) - 28",
    "MANGANESO (MN) - 29",
    "FÃ“SFORO (P) - 34",
    "ZINC (ZN) - 40",
    "CÃ“DIGO ISO (4/6/14) - 47",
    "CONTEO PARTÃCULAS >= 4 ÎœM - 49",
    "CONTEO PARTÃCULAS >= 6 ÎœM - 50",
    "CONTEO PARTÃCULAS >= 14 ÎœM - 48",
    "**OXIDACIÃ“N - 80",
    "**NITRACIÃ“N - 82",
    "NÃšMERO ÃCIDO (AN) - 43",
    "NÃšMERO BÃSICO (BN) - 12",
    "NÃšMERO BÃSICO (BN) - 17",
    "**HOLLÃN - 79",
    "DILUCIÃ“N POR COMBUSTIBLE - 46",
    "**AGUA (IR) - 81",
    "CONTENIDO AGUA (KARL FISCHER) - 41",
    "CONTENIDO GLICOL  - 105",
    "VISCOSIDAD A 100 Â°C - 13",
    "VISCOSIDAD A 40 Â°C - 14",
    "COLORIMETRÃA MEMBRANA DE PARCHE (MPC) - 51",
    "AGUA CUALITATIVA (PLANCHA) - 360",
    "AGUA LIBRE - 416",
    "ANÃLISIS ANTIOXIDANTES (AMINA) - 44",
    "ANÃLISIS ANTIOXIDANTES (FENOL) - 45",
    "COBRE (CU) - 119",
    "ESPUMA SEC 1 - ESTABILIDAD - 60",
    "ESPUMA SEC 1 - TENDENCIA - 59",
    "ESTAÃ‘O (SN) - 37",
    "**ÃNDICE VISCOSIDAD - 359",
    "RPVOT - 10",
    "SEPARABILIDAD AGUA A 54 Â°C (ACEITE) - 6",
    "SEPARABILIDAD AGUA A 54 Â°C (AGUA) - 7",
    "SEPARABILIDAD AGUA A 54 Â°C (EMULSIÃ“N) - 8",
    "SEPARABILIDAD AGUA A 54 Â°C (TIEMPO) - 83",
    "**ULTRACENTRÃFUGA (UC) - 1",
    "Archivo_Origen"          # Ãºltima fija
]

# â€”â€”â€”â€”â€”â€” Subida de mÃºltiples archivos â€”â€”â€”â€”â€”â€”
uploaded_files = st.file_uploader(
    "ðŸ“¤ Sube uno o varios archivos Excel (.xlsx):",
    type="xlsx",
    accept_multiple_files=True
)

if uploaded_files:
    # Unir todo como texto
    dfs = []
    for uploaded in uploaded_files:
        df = pd.read_excel(uploaded, header=0, dtype=str, engine="openpyxl")
        df["Archivo_Origen"] = uploaded.name
        dfs.append(df)
    df_global = pd.concat(dfs, ignore_index=True)

    # Columnas reales y mapas auxiliares
    columnas_reales = [c.strip() for c in df_global.columns.tolist()]
    mapa_nombre_a_indice = {col: i for i, col in enumerate(columnas_reales)}
    mapa_norm_a_nombre = {normalize_header(col): col for col in columnas_reales}
    expected_set_norm = {normalize_header(v) for v in expected_names}

    # â€”â€” Reporte de desalineaciones â€”â€”
    des_rows = []
    for pos_esp, esperado in enumerate(expected_names):
        letra_esp = col_index_to_letter(pos_esp)
        if esperado in mapa_nombre_a_indice:
            pos_real = mapa_nombre_a_indice[esperado]
            if pos_real != pos_esp:
                des_rows.append({
                    "PosiciÃ³n esperada": f"{pos_esp+1} ({letra_esp})",
                    "Encabezado esperado": esperado,
                    "PosiciÃ³n encontrada": f"{pos_real+1} ({col_index_to_letter(pos_real)})",
                })
        else:
            norm = normalize_header(esperado)
            if norm in mapa_norm_a_nombre:
                casi = mapa_norm_a_nombre[norm]
                pos_real = mapa_nombre_a_indice[casi]
                des_rows.append({
                    "PosiciÃ³n esperada": f"{pos_esp+1} ({letra_esp})",
                    "Encabezado esperado": esperado,
                    "PosiciÃ³n encontrada": f"{pos_real+1} ({col_index_to_letter(pos_real)}) â€“ (variante '{casi}')",
                })
            else:
                des_rows.append({
                    "PosiciÃ³n esperada": f"{pos_esp+1} ({letra_esp})",
                    "Encabezado esperado": esperado,
                    "PosiciÃ³n encontrada": "(no existe)",
                })

    st.subheader("ðŸ“‹ Tabla de Desalineaciones")
    if des_rows:
        st.dataframe(pd.DataFrame(des_rows), use_container_width=True)
    else:
        st.success("âœ… Todas las columnas estÃ¡n en la posiciÃ³n esperada.")

    st.divider()

    # â€”â€” Columnas no mapeadas con datos â€”â€”
    st.subheader("ðŸŸ  Columnas con datos no mapeadas (se agregarÃ¡n al final)")
    extra_rows = []
    extra_cols_ordered = []
    for idx, nombre in enumerate(columnas_reales):
        if normalize_header(nombre) not in expected_set_norm:
            datos = df_global.iloc[:, idx].notna().sum()
            if datos > 0:
                extra_rows.append({
                    "Letra actual": col_index_to_letter(idx),
                    "Encabezado no considerado": nombre,
                    "Registros con datos": int(datos),
                })
                extra_cols_ordered.append(nombre)

    if extra_rows:
        st.dataframe(pd.DataFrame(extra_rows), use_container_width=True)
    else:
        st.info("No se encontraron columnas adicionales con datos.")

    st.divider()

    # â€”â€” ConstrucciÃ³n del archivo final â€”â€”
    st.subheader("ðŸ§© ConstrucciÃ³n del archivo final (orden fijo + extras al final)")

    columnas_finales = []
    for esperado in expected_names:
        if esperado in mapa_nombre_a_indice:
            columnas_finales.append(df_global.iloc[:, mapa_nombre_a_indice[esperado]].rename(esperado))
        else:
            columnas_finales.append(pd.Series([None]*len(df_global), name=esperado))

    for nombre in extra_cols_ordered:
        if nombre not in [s.name for s in columnas_finales]:
            columnas_finales.append(df_global[nombre])

    df_resultado = pd.concat(columnas_finales, axis=1)

    st.subheader("ðŸ“‹ Vista previa â€“ Archivo Final")
    st.dataframe(df_resultado.head(10), use_container_width=True)
    make_downloads(df_resultado, "archivo_consolidado", sheet="Consolidado")



