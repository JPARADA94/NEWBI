import streamlit as st
import pandas as pd
from io import BytesIO
import unicodedata

# ‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî Configuraci√≥n general ‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî
st.set_page_config(page_title="Validaci√≥n Global ‚Äì Excel Consolidado v4.2", layout="wide")
st.title("üìä Validaci√≥n Global de Encabezados ‚Äì Excel Consolidado ‚Äì Mobil v4.2")
st.markdown("**Responsables:** Grupo de Soporte en Campo ‚Äì Mobil")

st.markdown(
    """
### üßæ Instrucciones de uso:
1. Sube **uno o varios archivos Excel (.xlsx)**.
2. El sistema unir√° todos los archivos en un solo conjunto.
3. Validar√° los encabezados sobre el conjunto completo.
4. Generar√° dos reportes:
   - **Tabla de desalineaciones**: ubicaci√≥n original, encabezado esperado, lo encontrado y nueva ubicaci√≥n del esperado.
   - **Tabla de columnas con datos no mapeadas**.
5. Permite **incluir columnas extra con datos**.
6. Genera **un √∫nico archivo Excel consolidado** y los reportes descargables.
"""
)

# ‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî Utilitarios ‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî
def col_letter_to_index(letter: str) -> int:
    idx = 0
    for c in letter.upper():
        idx = idx * 26 + (ord(c) - ord("A") + 1)
    return idx - 1


def col_index_to_letter(idx: int) -> str:
    letter = ""
    while idx >= 0:
        letter = chr(idx % 26 + ord('A')) + letter
        idx = idx // 26 - 1
    return letter


def normalize_header(s: str) -> str:
    if s is None:
        return ""
    s = s.strip()
    s = s.replace("‚â•", ">=").replace("Œú", "¬µ").replace("¬†", " ")  # NBSP a espacio
    s = s.replace("**", "")
    s = unicodedata.normalize('NFKD', s)
    s = "".join(ch for ch in s if not unicodedata.combining(ch))
    return s.lower()


def df_to_xlsx_bytes(df: pd.DataFrame, sheet: str = "Hoja") -> BytesIO:
    """Convierte un DataFrame a bytes XLSX usando openpyxl (sin XlsxWriter)."""
    buf = BytesIO()
    with pd.ExcelWriter(buf, engine="openpyxl") as w:
        df.to_excel(w, index=False, sheet_name=sheet)
    buf.seek(0)
    return buf


def make_downloads(df: pd.DataFrame, base_name: str, sheet: str):
    """Muestra botones de descarga CSV/XLSX para un DataFrame."""
    csv_bytes = df.to_csv(index=False).encode("utf-8-sig")
    xlsx_bytes = df_to_xlsx_bytes(df, sheet=sheet)
    c1, c2 = st.columns(2)
    c1.download_button(
        f"üì• {base_name} (CSV)", data=csv_bytes, file_name=f"{base_name}.csv", mime="text/csv"
    )
    c2.download_button(
        f"üì• {base_name} (XLSX)", data=xlsx_bytes, file_name=f"{base_name}.xlsx",
        mime="application/vnd.openxmlformats-officedocument.spreadsheetml.sheet"
    )


# ‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî Diccionario actualizado de columnas esperadas (ajustado) ‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî
columnas_esperadas = {
    "A": "NOMBRE_CLIENTE",
    "B": "NOMBRE_OPERACION",
    "C": "N_MUESTRA",
    "D": "CORRELATIVO",
    "E": "FECHA_MUESTREO",
    "F": "FECHA_INGRESO",
    "G": "FECHA_RECEPCION",
    "H": "FECHA_INFORME",
    "I": "EDAD_COMPONENTE",
    "J": "UNIDAD_EDAD_COMPONENTE",
    "K": "EDAD_PRODUCTO",
    "L": "UNIDAD_EDAD_PRODUCTO",
    "M": "CANTIDAD_ADICIONADA",
    "N": "UNIDAD_CANTIDAD_ADICIONADA",
    "O": "PRODUCTO",
    "U": "COMPONENTE",
    "V": "MARCA_COMPONENTE",
    "W": "MODELO_COMPONENTE",
    "X": "DESCRIPTOR_COMPONENTE",
    "Y": "ESTADO",
    "Z": "NIVEL_DE_SERVICIO",
    "IQ": "√çNDICE PQ (PQI) - 3",
    "MK": "PLATA (AG) - 19",
    "AK": "ALUMINIO (AL) - 20",
    "FM": "CROMO (CR) - 24",
    "BX": "COBRE (CU) - 25",
    "IF": "HIERRO (FE) - 26",
    "PB": "TITANIO (TI) - 38",
    "MN": "PLOMO (PB) - 35",
    "JS": "N√çQUEL (NI) - 32",
    "JM": "MOLIBDENO (MO) - 30",
    "OE": "SILICIO (SI) - 36",
    "OH": "SODIO (NA) - 31",
    "MP": "POTASIO (K) - 27",
    "PF": "VANADIO (V) - 39",
    "BK": "BORO (B) - 18",
    "BE": "BARIO (BA) - 21",
    "BO": "CALCIO (CA) - 22",
    "BM": "CADMIO (CD) - 23",
    "JG": "MAGNESIO (MG) - 28",
    "JH": "MANGANESO (MN) - 29",
    "HR": "F√ìSFORO (P) - 34",
    "PQ": "ZINC (ZN) - 40",
    "CA": "C√ìDIGO ISO (4/6/14) - 47",
    "FC": "CONTEO PART√çCULAS >= 4 ŒúM - 49",
    "FD": "CONTEO PART√çCULAS >= 6 ŒúM - 50",
    "FB": "CONTEO PART√çCULAS >= 14 ŒúM - 48",
    "KD": "**OXIDACI√ìN - 80",
    "JT": "**NITRACI√ìN - 82",
    "JW": "N√öMERO √ÅCIDO (AN) - 43",
    "JY": "N√öMERO B√ÅSICO (BN) - 12",
    "JX": "N√öMERO B√ÅSICO (BN) - 17",
    "IH": "**HOLL√çN - 79",
    "GP": "DILUCI√ìN POR COMBUSTIBLE - 46",
    "AF": "**AGUA (IR) - 81",
    "CT": "CONTENIDO AGUA (KARL FISCHER) - 41",
    "ES": "CONTENIDO GLICOL  - 105",
    "PI": "VISCOSIDAD A 100 ¬∞C - 13",
    "PJ": "VISCOSIDAD A 40 ¬∞C - 14",
    "CF": "COLORIMETR√çA MEMBRANA DE PARCHE (MPC) - 51",
    "AE": "AGUA CUALITATIVA (PLANCHA) - 360",
    "AH": "AGUA LIBRE - 416",
    "AL": "AN√ÅLISIS ANTIOXIDANTES (AMINA) - 44",
    "AM": "AN√ÅLISIS ANTIOXIDANTES (FENOL) - 45",
    "BW": "COBRE (CU) - 119",
    "GU": "ESPUMA SEC 1 - ESTABILIDAD - 60",
    "GV": "ESPUMA SEC 1 - TENDENCIA - 59",
    "HL": "ESTA√ëO (SN) - 37",
    "IT": "**√çNDICE VISCOSIDAD - 359",
    "NX": "RPVOT - 10",
    "NZ": "SEPARABILIDAD AGUA A 54 ¬∞C (ACEITE) - 6",
    "OA": "SEPARABILIDAD AGUA A 54 ¬∞C (AGUA) - 7",
    "OB": "SEPARABILIDAD AGUA A 54 ¬∞C (EMULSI√ìN) - 8",
    "OC": "SEPARABILIDAD AGUA A 54 ¬∞C (TIEMPO) - 83",
    "PE": "**ULTRACENTR√çFUGA (UC) - 1",
}

# ‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî Subida de m√∫ltiples archivos ‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî
uploaded_files = st.file_uploader(
    "üì§ Sube uno o varios archivos Excel (.xlsx):",
    type="xlsx",
    accept_multiple_files=True
)

if uploaded_files:
    # Unir todo como texto
    dfs = []
    for uploaded in uploaded_files:
        df = pd.read_excel(uploaded, header=0, dtype=str, engine="openpyxl")
        df["Archivo_Origen"] = uploaded.name
        dfs.append(df)
    df_global = pd.concat(dfs, ignore_index=True)

    columnas_reales = [c.strip() for c in df_global.columns.tolist()]
    expected_names = list(columnas_esperadas.values())

    # ‚Äî‚Äî Reporte de desalineaciones ‚Äî‚Äî
    des_rows = []
    for letra, esperado in columnas_esperadas.items():
        idx = col_letter_to_index(letra)
        if idx < len(columnas_reales):
            encontrado = columnas_reales[idx]
            if encontrado != esperado:
                nueva_letra = col_index_to_letter(columnas_reales.index(esperado)) if esperado in columnas_reales else "‚Äî"
                des_rows.append({
                    "Ubicaci√≥n original": letra,
                    "Encabezado esperado": esperado,
                    "Encontrado en origen": encontrado,
                    "Nueva ubicaci√≥n del esperado": nueva_letra,
                })
        else:
            des_rows.append({
                "Ubicaci√≥n original": letra,
                "Encabezado esperado": esperado,
                "Encontrado en origen": "(no existe)",
                "Nueva ubicaci√≥n del esperado": "‚Äî",
            })

    st.subheader("üìã Tabla de Desalineaciones")
    if des_rows:
        df_des = pd.DataFrame(des_rows, columns=[
            "Ubicaci√≥n original","Encabezado esperado","Encontrado en origen","Nueva ubicaci√≥n del esperado"
        ])
        st.dataframe(df_des, use_container_width=True)
        make_downloads(df_des, "reporte_desalineaciones", sheet="Desalineaciones")
    else:
        st.success("‚úÖ Todas las columnas coinciden con lo esperado.")

    st.divider()

    # ‚Äî‚Äî Columnas no mapeadas con datos ‚Äî‚Äî
    st.subheader("üü† Columnas con datos que no estaban en el mapa")
    expected_set_norm = {normalize_header(v) for v in columnas_esperadas.values()}
    extra_rows = []
    for idx, nombre in enumerate(columnas_reales):
        if normalize_header(nombre) not in expected_set_norm:
            datos = df_global.iloc[:, idx].notna().sum()
            if datos > 0:
                extra_rows.append({
                    "Letra": col_index_to_letter(idx),
                    "Encabezado no considerado": nombre,
                    "Registros con datos": int(datos),
                })
    if extra_rows:
        df_extra = pd.DataFrame(extra_rows, columns=["Letra","Encabezado no considerado","Registros con datos"])
        st.dataframe(df_extra, use_container_width=True)
        make_downloads(df_extra, "no_mapeadas_con_datos", sheet="No_mapeadas")
    else:
        st.info("No se encontraron columnas adicionales con datos.")

    st.divider()

    # ‚Äî‚Äî Construcci√≥n del archivo final por NOMBRE ‚Äî‚Äî
    st.subheader("üß© Construcci√≥n del archivo final")
    usar_normalizado = st.checkbox("Sugerir coincidencias usando comparaci√≥n normalizada (aproximada)", value=False)

    mapa_nombre_a_indice = {col: i for i, col in enumerate(columnas_reales)}
    mapa_norm_a_nombre = {normalize_header(col): col for col in columnas_reales}

    columnas_finales = []
    faltantes = []
    sugerencias = []
    for esperado in expected_names:
        if esperado in mapa_nombre_a_indice:
            columnas_finales.append(df_global.iloc[:, mapa_nombre_a_indice[esperado]].rename(esperado))
        else:
            if usar_normalizado:
                norm = normalize_header(esperado)
                if norm in mapa_norm_a_nombre:
                    casi = mapa_norm_a_nombre[norm]
                    sugerencias.append({"Esperado": esperado, "Coincidencia aproximada": casi})
                    columnas_finales.append(df_global.iloc[:, mapa_nombre_a_indice[casi]].rename(esperado))
                else:
                    faltantes.append(esperado)
                    columnas_finales.append(pd.Series([None]*len(df_global), name=esperado))
            else:
                faltantes.append(esperado)
                columnas_finales.append(pd.Series([None]*len(df_global), name=esperado))

    df_resultado = pd.concat(columnas_finales, axis=1)

    # Incluir columnas extra seleccionadas
    st.subheader("üìå Columnas extra con datos para incluir en el final (opcional)")
    if extra_rows:
        opciones_extra = {f"{r['Letra']} ‚Äì {r['Encabezado no considerado']}": r['Letra'] for r in extra_rows}
        seleccionadas = st.multiselect("Selecciona las columnas extra a incluir:", options=list(opciones_extra.keys()))
        if seleccionadas:
            letras_sel = [opciones_extra[s] for s in seleccionadas]
            idx_sel = [col_letter_to_index(L) for L in letras_sel]
            df_resultado = pd.concat([df_resultado, df_global.iloc[:, idx_sel]], axis=1)
    else:
        st.caption("No hay columnas extra con datos disponibles para a√±adir.")

    # A√±adir origen
    if "Archivo_Origen" in df_global.columns:
        df_resultado["Archivo_Origen"] = df_global["Archivo_Origen"]

    st.subheader("üìã Vista previa ‚Äì Archivo Final")
    st.dataframe(df_resultado.head(10), use_container_width=True)
    make_downloads(df_resultado, "archivo_consolidado", sheet="Consolidado")

    # Mostrar sugerencias/faltantes si aplica
    if sugerencias:
        with st.expander("Coincidencias aproximadas aplicadas"):
            st.write(pd.DataFrame(sugerencias))
    if faltantes:
        with st.expander("Encabezados faltantes en los archivos cargados"):
            st.write(pd.DataFrame({"Esperado": faltantes}))
